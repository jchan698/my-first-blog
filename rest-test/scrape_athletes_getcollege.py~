#!/usr/bin/env python
import sys
import requests
import lxml
import lxml.html
import json

#import html_scrape # YOU DONT NEED THIS

'''
html -> raw html text
elem -> an lxml.ElementTree object
page_elem -> an elem containing the entire parsed html page
athlete_elem -> an elem containing a row in the player table
athlete_elems -> list of athlete_elem(s)
athlete -> a dict of properties of an athlete 
athletes -> a list of athlete(s)
'''

def download_html_from_remote():
    return requests.get('http://opendorse.com/blog/top-100-highest-paid-athlete-endorsers-of-2013/').text

def get_element_from_html(html):
    return lxml.html.fromstring(html)


def find_table_on_page(page_element):
    athletes = []
    for index in xrange(2,109):	
        # tables = page_element.xpath("//*[@id='post-2673']/div/h3[index]/")   # OLD
        tables   = page_element.xpath("//*[@id='post-2673']/div/h3[" + str(index) + "]") 
        athletes.append(lxml.html.tostring(tables[0]))
    return athletes

html = download_html_from_remote()
page = get_element_from_html(html)
athletes = find_table_on_page(page)
for a in athletes:
    print a

# todo: upload the data to s3 either as a separate table or "merge" into the existing athletes table from postgres.
# https://docs.python.org/2/library/csv.html#module-contents
# >>> import csv
# >>> with open('eggs.csv', 'rb') as csvfile:
# ...     spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')
# ...     for row in spamreader:
# ...         print ', '.join(row)
# Spam, Spam, Spam, Spam, Spam, Baked Beans
# Spam, Lovely Spam, Wonderful Spam
