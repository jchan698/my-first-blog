#!/usr/bin/env python
import sys
import requests
import lxml
import lxml.html
import json

import html_scrape

'''
html -> raw html text
elem -> an lxml.ElementTree object
page_elem -> an elem containing the entire parsed html page
athlete_elem -> an elem containing a row in the player table
athlete_elems -> list of athlete_elem(s)
athlete -> a dict of properties of an athlete 
athletes -> a list of athlete(s)
'''

def download_html_from_remote():
    return requests.get('http://www.basketball-reference.com/players/a/').text

def get_element_from_html(html):
    return lxml.html.fromstring(html)

def find_table_on_page(page_element):
    tables = page_element.xpath("//*[@id='main']/div[5]/table")
    return tables[0]

def get_athlete_elems_from_table(table):
    return html_scrape.get_row_elements_from_table_element(table)

def get_athletes_from_athlete_elems(elements):
    return [get_athlete_from_athlete_elem(element) for element in elements]
        
def get_athlete_from_athlete_elem(athlete_elem):
     column_codes = [
         # key             index  xpath       attrib (None => lambda elem: elem.text)
         # ---             -----  -----       --------------------------------------
         #('name',          0,     '/a',       None),
         #('url',           0,     '/a',       lambda elem: elem.attrib['href']),
         #('dates',         0,     '/span[2]', None),
         #('position',      1,     '',         None),
         #('team',          2,     '/span',    None),
         #('age',           3,     '/span',    None),
         #('years',         4,     '/span',    None),
         #('dollars',       5,     '/span',    None),
         #('gteed_dollars', 6,     '/span',    None),
         #('gteed_pct',     7,     '/span',    None),
         #('average',       8,     '/span',    None),
         #('free_agent',    9,     '/span',    None),
         ('Player',          0,     '/a',       None),
         ('url',           0,     '/a',       lambda elem: elem.attrib['href']),
         ('College',      1,     '',         None),
 
     ]
     return html_scrape.get_dict_from_row_element(athlete_elem, column_codes)

def main():
    html = download_html_from_remote()
    page = get_element_from_html(html)
    table = find_table_on_page(page)
    athlete_elems = get_athlete_elems_from_table(table)
    athletes = get_athletes_from_athlete_elems(athlete_elems)
    print json.dumps(athletes)

    # todo: upload the data to s3 either as a separate table or "merge" into the existing athletes table from postgres.

if __name__ == "__main__":
    sys.exit(main())